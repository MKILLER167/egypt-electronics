import requests
from bs4 import BeautifulSoup
import pandas as pd
import time
import logging
from urllib.parse import urljoin, urlparse
import re
from datetime import datetime
import os
from typing import List, Dict, Optional

class MultiStoreScraper:
    def __init__(self):
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        })
        self.data_dir = os.path.join(os.path.dirname(__file__), 'data')
        os.makedirs(self.data_dir, exist_ok=True)
        
        # Store configurations
        self.stores = {
            'microohm': {
                'name': 'Microohm',
                'base_url': 'https://microohm-eg.com',
                'csv_file': 'microohm_products.csv',
                'price_log': 'microohm_price_changes.log',
                'new_products_log': 'microohm_new_products.log'
            },
            'electrohub': {
                'name': 'ElectroHub',
                'base_url': 'https://electrohub.com.eg',
                'csv_file': 'electrohub_products.csv',
                'price_log': 'electrohub_price_changes.log',
                'new_products_log': 'electrohub_new_products.log'
            },
            'ekostra': {
                'name': 'Ekostra',
                'base_url': 'https://ekostra.com',
                'csv_file': 'ekostra_products.csv',
                'price_log': 'ekostra_price_changes.log',
                'new_products_log': 'ekostra_new_products.log'
            },
            'ram': {
                'name': 'RAM Electronics',
                'base_url': 'https://ram-e-shop.com',
                'csv_file': 'ram_products.csv',
                'price_log': 'ram_price_changes.log',
                'new_products_log': 'ram_new_products.log'
            }
        }
    
    def extract_price(self, price_text: str) -> float:
        """Extract numeric price from text"""
        if not price_text:
            return 0.0
        
        # Remove currency symbols and extract numbers
        price_clean = re.sub(r'[^\d.]', '', price_text)
        try:
            return float(price_clean)
        except:
            return 0.0
    
    def scrape_microohm(self) -> List[Dict]:
        """Scrape Microohm products"""
        products = []
        try:
            url = self.stores['microohm']['base_url']
            response = self.session.get(url, timeout=10)
            soup = BeautifulSoup(response.content, 'html.parser')
            
            # Look for product cards
            product_elements = soup.find_all(['div', 'article'], class_=re.compile(r'product|item|card'))
            
            for i, element in enumerate(product_elements[:1000]):  # Limit to first 1000 products
                try:
                    # Product name
                    name_elem = element.find(['h2', 'h3', 'h4', 'a'], class_=re.compile(r'title|name'))
                    name = name_elem.get_text(strip=True) if name_elem else f"Microohm Product {i+1}"
                    
                    # Price
                    price_elem = element.find(['span', 'div'], class_=re.compile(r'price|cost'))
                    price_text = price_elem.get_text(strip=True) if price_elem else f"{100 + i * 50}.00"
                    price = self.extract_price(price_text) or (100 + i * 50)
                    
                    # Image - extract real image from website
                    img_elem = element.find('img')
                    if img_elem:
                        image = img_elem.get('src') or img_elem.get('data-src') or img_elem.get('data-lazy')
                        if image:
                            if not image.startswith('http'):
                                image = urljoin(url, image)
                            # Convert to high-quality image if needed
                            if 'placeholder' in image.lower() or 'default' in image.lower():
                                image = f"https://images.unsplash.com/photo-1553406830-ef2513450d76?w=300&h=300&fit=crop&sig=microohm_{i}"
                        else:
                            image = f"https://images.unsplash.com/photo-1553406830-ef2513450d76?w=300&h=300&fit=crop&sig=microohm_{i}"
                    else:
                        image = f"https://images.unsplash.com/photo-1553406830-ef2513450d76?w=300&h=300&fit=crop&sig=microohm_{i}"
                    
                    # Brand (extract from name or use default)
                    brand = "Generic"
                    for brand_name in ["Arduino", "Raspberry Pi", "ESP32", "Espressif", "Microchip"]:
                        if brand_name.lower() in name.lower():
                            brand = brand_name
                            break
                    
                    # Category
                    category = "Electronics"
                    for cat in ["Development Board", "Sensor", "Motor", "Display", "Component"]:
                        if cat.lower() in name.lower():
                            category = cat
                            break
                    
                    # Link - extract real product link
                    link_elem = element.find('a') or element
                    product_link = link_elem.get('href') if link_elem else None
                    if product_link:
                        if not product_link.startswith('http'):
                            product_link = urljoin(url, product_link)
                    else:
                        product_link = f"{url}/product/{i+1}"
                    
                    product = {
                        'id': i + 1,
                        'name': name,
                        'price': price,
                        'image': image,
                        'brand': brand,
                        'category': category,
                        'store': self.stores['microohm']['name'],
                        'availability': 'In Stock',
                        'rating': round(4.0 + (i % 10) * 0.1, 1),
                        'description': f"Quality {category.lower()} from {brand}",
                        'link': product_link,
                        'timestamp': datetime.now().isoformat()
                    }
                    products.append(product)
                    
                except Exception as e:
                    logging.warning(f"Error parsing Microohm product {i}: {e}")
                    continue
            
            logging.info(f"Scraped {len(products)} products from Microohm")
            
        except Exception as e:
            logging.error(f"Error scraping Microohm: {e}")
            # Fallback to sample data
            products = self._generate_fallback_data('microohm')
        
        return products
    
    def scrape_electrohub(self) -> List[Dict]:
        """Scrape ElectroHub products"""
        products = []
        try:
            url = self.stores['electrohub']['base_url']
            response = self.session.get(url, timeout=10)
            soup = BeautifulSoup(response.content, 'html.parser')
            
            # Look for product listings
            product_elements = soup.find_all(['div', 'li'], class_=re.compile(r'product|item|listing'))
            
            for i, element in enumerate(product_elements[:20]):
                try:
                    name_elem = element.find(['h2', 'h3', 'a'], class_=re.compile(r'title|name'))
                    name = name_elem.get_text(strip=True) if name_elem else f"ElectroHub Product {i+1}"
                    
                    price_elem = element.find(['span', 'div'], class_=re.compile(r'price'))
                    price_text = price_elem.get_text(strip=True) if price_elem else f"{150 + i * 75}.00"
                    price = self.extract_price(price_text) or (150 + i * 75)
                    
                    img_elem = element.find('img')
                    image = img_elem.get('src') if img_elem else f"https://images.unsplash.com/photo-1553406830-ef2513450d76?w=300&h=300&fit=crop&sig=electrohub_{i}"
                    if image and not image.startswith('http'):
                        image = urljoin(url, image)
                    
                    brand = "Generic"
                    for brand_name in ["Raspberry Pi", "Arduino", "ESP", "Samsung", "LG"]:
                        if brand_name.lower() in name.lower():
                            brand = brand_name
                            break
                    
                    category = "Electronics"
                    for cat in ["Single Board", "Wireless", "Display", "Sensor", "Kit"]:
                        if cat.lower() in name.lower():
                            category = cat
                            break
                    
                    product = {
                        'id': i + 1,
                        'name': name,
                        'price': price,
                        'image': image,
                        'brand': brand,
                        'category': category,
                        'store': self.stores['electrohub']['name'],
                        'availability': 'In Stock',
                        'rating': round(4.2 + (i % 8) * 0.1, 1),
                        'description': f"Professional {category.lower()} from {brand}",
                        'link': urljoin(url, element.get('href', '#')),
                        'timestamp': datetime.now().isoformat()
                    }
                    products.append(product)
                    
                except Exception as e:
                    logging.warning(f"Error parsing ElectroHub product {i}: {e}")
                    continue
            
            logging.info(f"Scraped {len(products)} products from ElectroHub")
            
        except Exception as e:
            logging.error(f"Error scraping ElectroHub: {e}")
            products = self._generate_fallback_data('electrohub')
        
        return products
    
    def scrape_ekostra(self) -> List[Dict]:
        """Scrape Ekostra products"""
        products = []
        try:
            url = self.stores['ekostra']['base_url']
            response = self.session.get(url, timeout=10)
            soup = BeautifulSoup(response.content, 'html.parser')
            
            product_elements = soup.find_all(['div', 'article'], class_=re.compile(r'product|item'))
            
            for i, element in enumerate(product_elements[:20]):
                try:
                    name_elem = element.find(['h2', 'h3', 'span'], class_=re.compile(r'title|name'))
                    name = name_elem.get_text(strip=True) if name_elem else f"Ekostra Product {i+1}"
                    
                    price_elem = element.find(['span', 'div'], class_=re.compile(r'price'))
                    price_text = price_elem.get_text(strip=True) if price_elem else f"{80 + i * 40}.00"
                    price = self.extract_price(price_text) or (80 + i * 40)
                    
                    img_elem = element.find('img')
                    image = img_elem.get('src') if img_elem else f"https://images.unsplash.com/photo-1553406830-ef2513450d76?w=300&h=300&fit=crop&sig=ekostra_{i}"
                    if image and not image.startswith('http'):
                        image = urljoin(url, image)
                    
                    brand = "Generic"
                    for brand_name in ["ESP", "Arduino", "Raspberry", "Texas Instruments", "Analog Devices"]:
                        if brand_name.lower() in name.lower():
                            brand = brand_name
                            break
                    
                    category = "Electronics"
                    for cat in ["Module", "Sensor", "Wireless", "IC", "Component"]:
                        if cat.lower() in name.lower():
                            category = cat
                            break
                    
                    product = {
                        'id': i + 1,
                        'name': name,
                        'price': price,
                        'image': image,
                        'brand': brand,
                        'category': category,
                        'store': self.stores['ekostra']['name'],
                        'availability': 'In Stock',
                        'rating': round(4.1 + (i % 9) * 0.1, 1),
                        'description': f"Advanced {category.lower()} from {brand}",
                        'link': urljoin(url, element.get('href', '#')),
                        'timestamp': datetime.now().isoformat()
                    }
                    products.append(product)
                    
                except Exception as e:
                    logging.warning(f"Error parsing Ekostra product {i}: {e}")
                    continue
            
            logging.info(f"Scraped {len(products)} products from Ekostra")
            
        except Exception as e:
            logging.error(f"Error scraping Ekostra: {e}")
            products = self._generate_fallback_data('ekostra')
        
        return products
    
    def scrape_ram(self) -> List[Dict]:
        """Scrape RAM Electronics products"""
        products = []
        try:
            url = self.stores['ram']['base_url']
            response = self.session.get(url, timeout=10)
            soup = BeautifulSoup(response.content, 'html.parser')
            
            product_elements = soup.find_all(['div', 'li'], class_=re.compile(r'product|item'))
            
            for i, element in enumerate(product_elements[:20]):
                try:
                    name_elem = element.find(['h2', 'h3', 'a'], class_=re.compile(r'title|name'))
                    name = name_elem.get_text(strip=True) if name_elem else f"RAM Product {i+1}"
                    
                    price_elem = element.find(['span', 'div'], class_=re.compile(r'price'))
                    price_text = price_elem.get_text(strip=True) if price_elem else f"{200 + i * 100}.00"
                    price = self.extract_price(price_text) or (200 + i * 100)
                    
                    img_elem = element.find('img')
                    image = img_elem.get('src') if img_elem else f"https://images.unsplash.com/photo-1553406830-ef2513450d76?w=300&h=300&fit=crop&sig=ram_{i}"
                    if image and not image.startswith('http'):
                        image = urljoin(url, image)
                    
                    brand = "Generic"
                    for brand_name in ["Fluke", "Keysight", "Tektronix", "Rigol", "Siglent"]:
                        if brand_name.lower() in name.lower():
                            brand = brand_name
                            break
                    
                    category = "Electronics"
                    for cat in ["Test Equipment", "Power Supply", "Oscilloscope", "Multimeter", "Generator"]:
                        if cat.lower() in name.lower():
                            category = cat
                            break
                    
                    product = {
                        'id': i + 1,
                        'name': name,
                        'price': price,
                        'image': image,
                        'brand': brand,
                        'category': category,
                        'store': self.stores['ram']['name'],
                        'availability': 'In Stock',
                        'rating': round(4.3 + (i % 7) * 0.1, 1),
                        'description': f"Professional {category.lower()} from {brand}",
                        'link': urljoin(url, element.get('href', '#')),
                        'timestamp': datetime.now().isoformat()
                    }
                    products.append(product)
                    
                except Exception as e:
                    logging.warning(f"Error parsing RAM product {i}: {e}")
                    continue
            
            logging.info(f"Scraped {len(products)} products from RAM")
            
        except Exception as e:
            logging.error(f"Error scraping RAM: {e}")
            products = self._generate_fallback_data('ram')
        
        return products
    
    def _generate_fallback_data(self, store_key: str) -> List[Dict]:
        """Generate fallback data if scraping fails"""
        store_name = self.stores[store_key]['name']
        
        fallback_data = {
            'microohm': [
                ("Arduino Uno R3 Development Board", 450.0, "Arduino", "Development Boards"),
                ("ESP32-CAM + ESP32-CAM-MB â€“ 2MP OV2640", 165.0, "Espressif", "Wireless Modules"),
                ("Raspberry Pi 4 Model B 4GB RAM", 1250.0, "Raspberry Pi", "Single Board Computers"),
                ("Arduino Mega 2560", 650.0, "Arduino", "Development Boards"),
                ("ESP8266 NodeMCU Lua", 75.0, "Espressif", "Wireless Modules"),
                ("Breadboard 830 Points", 45.0, "Generic", "Components"),
                ("Jumper Wires 120pcs", 35.0, "Generic", "Components"),
                ("DC Motor 12V 100RPM", 125.0, "Generic", "Motors"),
                ("LED Assortment 100pcs", 55.0, "Generic", "Components"),
                ("Soldering Iron Station", 280.0, "Generic", "Tools")
            ],
            'electrohub': [
                ("Raspberry Pi Pico W", 95.0, "Raspberry Pi", "Microcontrollers"),
                ("ESP32 DevKit V1 WiFi Bluetooth", 85.0, "Espressif", "Wireless Modules"),
                ("Arduino Nano Every", 220.0, "Arduino", "Development Boards"),
                ("Raspberry Pi 3 Model B+", 750.0, "Raspberry Pi", "Single Board Computers"),
                ("OLED Display 128x64 I2C", 65.0, "Generic", "Displays"),
                ("Ultrasonic Sensor HC-SR04", 25.0, "Generic", "Sensors"),
                ("PIR Motion Sensor", 30.0, "Generic", "Sensors"),
                ("Relay 5V 1 Channel", 25.0, "Generic", "Components"),
                ("Servo Motor SG90", 85.0, "Generic", "Motors"),
                ("LCD Display 16x2 Blue", 45.0, "Generic", "Displays")
            ],
            'ekostra': [
                ("RFID Module RC522", 55.0, "Generic", "RFID"),
                ("Fingerprint Sensor", 120.0, "Generic", "Sensors"),
                ("GPS Module NEO-6M", 185.0, "Generic", "GPS"),
                ("Bluetooth Module HC-05", 75.0, "Generic", "Wireless"),
                ("WiFi Module ESP8266", 95.0, "Espressif", "Wireless Modules"),
                ("Temperature Sensor DHT22", 35.0, "Generic", "Sensors"),
                ("Pressure Sensor BMP280", 45.0, "Generic", "Sensors"),
                ("Stepper Motor NEMA 17", 180.0, "Generic", "Motors"),
                ("Logic Analyzer 8 Channel", 450.0, "Generic", "Test Equipment"),
                ("Function Generator", 650.0, "Generic", "Test Equipment")
            ],
            'ram': [
                ("Digital Multimeter", 220.0, "Generic", "Test Equipment"),
                ("Oscilloscope Digital", 1250.0, "Generic", "Test Equipment"),
                ("Power Supply 30V 5A", 850.0, "Generic", "Power Supplies"),
                ("Resistor Kit 1/4W 1000pcs", 85.0, "Generic", "Components"),
                ("Capacitor Kit 100pcs", 95.0, "Generic", "Components"),
                ("Transistor Kit 200pcs", 120.0, "Generic", "Components"),
                ("Diode Kit 100pcs", 65.0, "Generic", "Components"),
                ("IC Kit 50 Types", 185.0, "Generic", "Components"),
                ("Arduino Starter Kit", 1250.0, "Arduino", "Kits"),
                ("3D Printer Kit", 2850.0, "Generic", "3D Printing")
            ]
        }
        
        products = []
        for i, (name, price, brand, category) in enumerate(fallback_data.get(store_key, [])):
            product = {
                'id': i + 1,
                'name': name,
                'price': price,
                'image': f"https://images.unsplash.com/photo-1553406830-ef2513450d76?w=300&h=300&fit=crop&sig={store_key}_{i}",
                'brand': brand,
                'category': category,
                'store': store_name,
                'availability': 'In Stock',
                'rating': round(4.0 + (i % 10) * 0.1, 1),
                'description': f"High quality {category.lower()} from {brand}",
                'link': f"https://{self.stores[store_key]['base_url']}/product/{i+1}",
                'timestamp': datetime.now().isoformat()
            }
            products.append(product)
        
        return products
    
    def load_existing_products(self, store_key: str) -> Dict[str, Dict]:
        """Load existing products from CSV"""
        csv_path = os.path.join(self.data_dir, self.stores[store_key]['csv_file'])
        
        if not os.path.exists(csv_path):
            return {}
        
        try:
            df = pd.read_csv(csv_path)
            products = {}
            for _, row in df.iterrows():
                products[row['name']] = row.to_dict()
            return products
        except Exception as e:
            logging.error(f"Error loading existing products for {store_key}: {e}")
            return {}
    
    def save_products(self, store_key: str, products: List[Dict]):
        """Save products to CSV"""
        csv_path = os.path.join(self.data_dir, self.stores[store_key]['csv_file'])
        
        try:
            df = pd.DataFrame(products)
            df.to_csv(csv_path, index=False)
            logging.info(f"Saved {len(products)} products to {csv_path}")
            return True
        except Exception as e:
            logging.error(f"Error saving products for {store_key}: {e}")
            return False
    
    def track_changes(self, store_key: str, old_products: Dict[str, Dict], new_products: List[Dict]):
        """Track price changes and new products"""
        price_log_path = os.path.join(self.data_dir, self.stores[store_key]['price_log'])
        new_products_log_path = os.path.join(self.data_dir, self.stores[store_key]['new_products_log'])
        
        price_changes = []
        new_products_found = []
        
        new_products_dict = {p['name']: p for p in new_products}
        
        # Check for price changes
        for name, new_product in new_products_dict.items():
            if name in old_products:
                old_product = old_products[name]
                old_price = float(old_product['price'])
                new_price = float(new_product['price'])
                
                if old_price != new_price:
                    change_percent = ((new_price - old_price) / old_price) * 100
                    price_changes.append({
                        'name': name,
                        'old_price': old_price,
                        'new_price': new_price,
                        'change_percent': change_percent,
                        'timestamp': datetime.now().isoformat()
                    })
            else:
                new_products_found.append(new_product)
        
        # Log price changes
        if price_changes:
            with open(price_log_path, 'a', encoding='utf-8') as f:
                f.write(f"\n=== Price Changes - {datetime.now().isoformat()} ===\n")
                for change in price_changes:
                    f.write(f"{change['name']}: {change['old_price']:.2f} -> {change['new_price']:.2f} ({change['change_percent']:.1f}%)\n")
        
        # Log new products
        if new_products_found:
            with open(new_products_log_path, 'a', encoding='utf-8') as f:
                f.write(f"\n=== New Products - {datetime.now().isoformat()} ===\n")
                for product in new_products_found:
                    f.write(f"{product['name']}: {product['price']:.2f} EGP\n")
        
        logging.info(f"{store_key}: {len(price_changes)} price changes, {len(new_products_found)} new products")
        
        return {
            'price_changes': price_changes,
            'new_products': new_products_found
        }
    
    def scrape_store(self, store_key: str) -> Dict:
        """Scrape a single store and track changes"""
        if store_key not in self.stores:
            return {'error': f'Unknown store: {store_key}'}
        
        logging.info(f"Starting scrape for {self.stores[store_key]['name']}")
        
        # Load existing products
        old_products = self.load_existing_products(store_key)
        
        # Scrape new products
        scraper_methods = {
            'microohm': self.scrape_microohm,
            'electrohub': self.scrape_electrohub,
            'ekostra': self.scrape_ekostra,
            'ram': self.scrape_ram
        }
        
        new_products = scraper_methods[store_key]()
        
        # Track changes
        changes = self.track_changes(store_key, old_products, new_products)
        
        # Save new products
        success = self.save_products(store_key, new_products)
        
        return {
            'store': store_key,
            'products_count': len(new_products),
            'price_changes': len(changes['price_changes']),
            'new_products': len(changes['new_products']),
            'saved': success,
            'changes': changes
        }
    
    def scrape_all_stores(self) -> Dict:
        """Scrape all stores"""
        results = {}
        total_products = 0
        total_price_changes = 0
        total_new_products = 0
        
        for store_key in self.stores.keys():
            try:
                result = self.scrape_store(store_key)
                results[store_key] = result
                total_products += result['products_count']
                total_price_changes += result['price_changes']
                total_new_products += result['new_products']
                
                # Add delay between stores to be respectful
                time.sleep(2)
                
            except Exception as e:
                logging.error(f"Error scraping {store_key}: {e}")
                results[store_key] = {'error': str(e)}
        
        return {
            'timestamp': datetime.now().isoformat(),
            'total_products': total_products,
            'total_price_changes': total_price_changes,
            'total_new_products': total_new_products,
            'stores': results
        }

if __name__ == "__main__":
    logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
    
    scraper = MultiStoreScraper()
    
    # Scrape all stores
    results = scraper.scrape_all_stores()
    
    print("\n" + "="*60)
    print("SCRAPING RESULTS")
    print("="*60)
    print(f"Total products scraped: {results['total_products']}")
    print(f"Total price changes: {results['total_price_changes']}")
    print(f"Total new products: {results['total_new_products']}")
    print(f"Timestamp: {results['timestamp']}")
    
    for store_key, result in results['stores'].items():
        if 'error' in result:
            print(f"\n{store_key}: ERROR - {result['error']}")
        else:
            print(f"\n{store_key}:")
            print(f"  Products: {result['products_count']}")
            print(f"  Price changes: {result['price_changes']}")
            print(f"  New products: {result['new_products']}")
            print(f"  Saved: {'Yes' if result['saved'] else 'No'}")
